{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import tifffile as tiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATOS_train\n",
    "datos_train = pd.read_csv(\"data/DATOS_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_feats = \"data/train_features/content/train_features/\"\n",
    "path_labels = \"data/train_labels/content/train_labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = []\n",
    "labels = []\n",
    "for n in range(len(datos_train)):\n",
    "    path_feat = datos_train.iloc[n].feature_image\n",
    "    path_label = datos_train.iloc[n].label_image\n",
    "    feat = tiff.imread(path_feats + path_feat)\n",
    "    label = tiff.imread(path_labels + path_label)\n",
    "    \n",
    "    feats.append(feat)\n",
    "    labels.append(label)\n",
    "    \n",
    "feats = np.array(feats)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.min(), feats.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.min(), labels.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(labels.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepocess Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.min(feats[1][:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distribution per layer\n",
    "\n",
    "# for i in range(10):\n",
    "#     mins = []\n",
    "#     maxs = []\n",
    "#     for j in range(feats.shape[0]):\n",
    "#         vals = feats[j][:, :, i].flatten()\n",
    "#         min_ = vals.min()\n",
    "#         max_ = vals.max()\n",
    "    \n",
    "#         mins.append(min_)\n",
    "#         maxs.append(max_)\n",
    "\n",
    "#     f, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "#     ax[0].hist(mins)\n",
    "#     ax[0].set_title(f'Mins for layer {i}')\n",
    "#     ax[1].hist(maxs)\n",
    "#     ax[1].set_title(f'Maxs for layer {i}')\n",
    "#     plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # which have minimum values < - 1?\n",
    "# outliers = []\n",
    "# for i in feats:\n",
    "#     if np.min(i) < -1:\n",
    "#         outliers.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we remove outliers\n",
    "feats_final = []\n",
    "labels_final0 = []\n",
    "\n",
    "for n in range(len(datos_train)):\n",
    "    path_feat = datos_train.iloc[n].feature_image\n",
    "    path_label = datos_train.iloc[n].label_image\n",
    "    \n",
    "    feat = tiff.imread(path_feats + path_feat)\n",
    "    label = tiff.imread(path_labels + path_label)\n",
    "    \n",
    "    if feat.min() >= -1:\n",
    "        feats_final.append(feat)\n",
    "        labels_final0.append(label)\n",
    "    \n",
    "feats_final = np.array(feats_final)\n",
    "labels_final0 = np.array(labels_final0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_final.shape, labels_final0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distribution per layer\n",
    "\n",
    "# for i in range(10):\n",
    "#     mins = []\n",
    "#     maxs = []\n",
    "#     for j in range(feats_final.shape[0]):\n",
    "#         vals = feats_final[j][:, :, i].flatten()\n",
    "#         min_ = vals.min()\n",
    "#         max_ = vals.max()\n",
    "    \n",
    "#         mins.append(min_)\n",
    "#         maxs.append(max_)\n",
    "\n",
    "#     f, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "#     ax[0].hist(mins)\n",
    "#     ax[0].set_title(f'Mins for layer {i}')\n",
    "#     ax[1].hist(maxs)\n",
    "#     ax[1].set_title(f'Maxs for layer {i}')\n",
    "#     plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_final0[0].shape\n",
    "labels_final0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess label - scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_final = labels_final0/7\n",
    "# labels_final.min(), labels_final.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess label (separate per class)\n",
    "Optional. We split each prediction into a 128 X 128 X 8 tensor. Each of the 8 layers correspond to classes 0, 1, 2, 4, 5, 6, 7, 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_feat = feats_final[20]\n",
    "sample_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_feat[:,:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_label = labels_final0[20]\n",
    "sample_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_array(sample_label):\n",
    "    new_array = []\n",
    "\n",
    "    for i in [0, 1, 2, 4, 5, 6, 7, 15]:\n",
    "        layer = sample_label==i\n",
    "        layer = layer.astype(int)\n",
    "        new_array.append(layer)\n",
    "    new_array = np.array(new_array)\n",
    "    new_array = np.swapaxes(new_array, 0, 2)\n",
    "    new_array = np.rot90(new_array, k=-1)\n",
    "    new_array = np.fliplr(new_array)\n",
    "    \n",
    "    \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_split = split_array(sample_label)\n",
    "for i in range(8):\n",
    "    plt.imshow(sample_split[:, :, i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_final = np.array([split_array(i) for i in labels_final0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_final.shape, labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compress labels\n",
    "\n",
    "def compress_label(matrix):\n",
    "    classes = [0, 1, 2, 4, 5, 6, 7, 15]\n",
    "\n",
    "    new_array = []\n",
    "    for i in range(8):\n",
    "        layer = matrix[:, :, i]\n",
    "        layer = np.round(layer) * classes[i]\n",
    "        new_array.append(layer)\n",
    "    \n",
    "    new_array = np.array(new_array)\n",
    "    new_array = np.swapaxes(new_array, 0, 2)\n",
    "    new_array = np.max(new_array, axis=2)\n",
    "    new_array = np.rot90(new_array, k=-1)\n",
    "    new_array = np.fliplr(new_array)\n",
    "    \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labels_final0[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(compress_label(labels_final[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of defining a 70x70 patchgan discriminator model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder and Decoder Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an encoder block\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add downsampling layer\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # conditionally add batch normalization\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    # leaky relu activation\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g\n",
    " \n",
    "# define a decoder block\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add upsampling layer\n",
    "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # add batch normalization\n",
    "    g = BatchNormalization()(g, training=True)\n",
    "    # conditionally add dropout\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g, training=True)\n",
    "    # merge with skip connection\n",
    "    g = Concatenate()([g, skip_in])\n",
    "    # relu activation\n",
    "    g = Activation('relu')(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation='sigmoid'\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape=(128,128,10)):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "    # encoder model: C64-C128-C256\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    e3 = define_encoder_block(e2, 256)\n",
    "    # bottleneck, no batch norm and relu\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e3)\n",
    "    b = Activation('relu')(b)\n",
    "    # decoder model: CD512-CD1024-CD1024-C1024-C1024-C512-C256-C128\n",
    "    d1 = decoder_block(b, e3, 256, dropout=False)\n",
    "    d2 = decoder_block(d1, e2, 128, dropout=False)\n",
    "    d3 = decoder_block(d2, e1, 64, dropout=False)\n",
    "    # output\n",
    "    g = Conv2DTranspose(8, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d3)\n",
    "    out_image = Activation(activation)(g)\n",
    "    # define model\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # unpack dataset\n",
    "    trainA, trainB = dataset\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, trainA.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X1, X2 = trainA[ix], trainB[ix]\n",
    "    \n",
    "    return [X1, X2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date for Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = str(date.today().year)+'_'+str(date.today().month)+'_'+str(date.today().day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define image shape\n",
    "image_shape = (128,128,10)\n",
    "# create the model\n",
    "unet_model = define_generator(image_shape)\n",
    "\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "# unet_model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[100])\n",
    "\n",
    "loss = 'categorical_crossentropy'\n",
    "unet_model.compile(loss=loss, optimizer=opt, loss_weights=[100])\n",
    "\n",
    "# summarize the model\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net with Adversarial Loss on Prediction Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WGANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We measure:\n",
    "<br>1) Average of intersection over Union for each of the classes\n",
    "<br>2) Pixel-by-pixel accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # k-fold cross validation function\n",
    "\n",
    "# def get_train_valid(num_samples, k):\n",
    "#     valid_inds = []\n",
    "#     train_inds = []\n",
    "    \n",
    "#     num_valid = num_samples // k\n",
    "#     num_train = num_samples - num_valid\n",
    "    \n",
    "#     for i in range(k):\n",
    "#         valid_ind = np.arange(num_valid * i, num_valid * i + num_valid)\n",
    "#         train_ind = np.array([i for i in np.arange(num_samples) if i not in valid_ind])\n",
    "        \n",
    "#         valid_inds.append(valid_ind)\n",
    "#         train_inds.append(train_ind)\n",
    "        \n",
    "#     return valid_inds, train_inds\n",
    "# #         print(valid_inds[0], valid_inds[-1])\n",
    "# #         print(len(valid_inds))\n",
    "# #         print(len(train_inds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pix2pix models\n",
    "def train(g_model, dataset, n_epochs=1, n_batch=1):\n",
    "    # unpack dataset\n",
    "    trainA, trainB = dataset\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    print(n_steps)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # select a batch of real samples\n",
    "        [X_realA, X_realB] = generate_real_samples(dataset, n_batch)\n",
    "        # update the generator\n",
    "        g_loss = g_model.train_on_batch(X_realA, X_realB)\n",
    "        # summarize performance\n",
    "        if i % 1000 == 0:\n",
    "            print('>%d, g[%.3f]' % (i+1, g_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional Cross Validation\n",
    "# num_samples = feats_final.shape[0]\n",
    "# k = 3\n",
    "# valid_inds, train_inds = get_train_valid(num_samples, k)\n",
    "\n",
    "# i = 0\n",
    "# train_feats = feats_final[train_inds[i]]\n",
    "# valid_feats = feats_final[valid_inds[i]]\n",
    "\n",
    "# train_labels = labels_final[train_inds[i]]\n",
    "# valid_labels = labels_final[valid_inds[i]]\n",
    "\n",
    "# train_feats.shape, valid_feats.shape, train_labels.shape, valid_labels.shape\n",
    "\n",
    "train_feats = feats_final\n",
    "train_labels = labels_final\n",
    "\n",
    "train_feats.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'datos_unet_gen_categorical_crossentropy_sigmoid_2019_11_17_0'\n",
    "unet_model.load_weights(f\"models/{file}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # augmentation combine random images together\n",
    "\n",
    "# train_feats_comb = []\n",
    "# train_labels_comb = []\n",
    "\n",
    "# for i in range(5000):\n",
    "#     to_combine = np.random.choice(range(len(train_feats)), size=2, replace=False)\n",
    "#     a_train = train_feats[to_combine[0]]\n",
    "#     b_train = train_feats[to_combine[1]]\n",
    "\n",
    "#     a_label = train_labels[to_combine[0]]\n",
    "#     b_label = train_labels[to_combine[1]]\n",
    "\n",
    "#     c_train = np.max([a_train, b_train], 0)\n",
    "#     c_label = np.max([a_label, b_label], 0)\n",
    "    \n",
    "#     train_feats_comb.append(c_train)\n",
    "#     train_labels_comb.append(c_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feats_comb = np.array(train_feats_comb)\n",
    "# train_labels_comb = np.array(train_labels_comb)\n",
    "\n",
    "# train_feats_comb.shape, train_labels_comb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow(a_train[:, :, 0])\n",
    "# plt.show()\n",
    "# plt.imshow(b_train[:, :, 0])\n",
    "# plt.show()\n",
    "# plt.imshow(a_label[:, :, 0])\n",
    "# plt.show()\n",
    "# plt.imshow(b_label[:, :, 0])\n",
    "# plt.show()\n",
    "# plt.imshow(c_train[:, :, 0])\n",
    "# plt.show()\n",
    "# plt.imshow(c_label[:, :, 0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# i = 3\n",
    "# plt.imshow(a_train[:, :, i])\n",
    "# plt.show()\n",
    "# plt.imshow(b_train[:, :, i])\n",
    "# plt.show()\n",
    "# plt.imshow(a_label[:, :, i])\n",
    "# plt.show()\n",
    "# plt.imshow(b_label[:, :, i])\n",
    "# plt.show()\n",
    "# plt.imshow(c_train[:, :, i])\n",
    "# plt.show()\n",
    "# plt.imshow(c_label[:, :, i])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get n random images from all images in the dataset\n",
    "    \n",
    "filename_g = f'models/datos_unet_gen_{loss}_{activation}_{today}'\n",
    "\n",
    "for j in range(10000):\n",
    "    \n",
    "    # data augmentation combine\n",
    "    comb = np.random.choice([0, 1, 1, 1])\n",
    "    if comb == 1:\n",
    "        \n",
    "        train_feats_comb = []\n",
    "        train_labels_comb = []\n",
    "\n",
    "        for i in range(5000):\n",
    "            to_combine = np.random.choice(range(len(train_feats)), size=2, replace=False)\n",
    "            a_train = train_feats[to_combine[0]]\n",
    "            b_train = train_feats[to_combine[1]]\n",
    "\n",
    "            a_label = train_labels[to_combine[0]]\n",
    "            b_label = train_labels[to_combine[1]]\n",
    "\n",
    "            c_train = np.max([a_train, b_train], 0)\n",
    "            c_label = np.max([a_label, b_label], 0)\n",
    "\n",
    "            train_feats_comb.append(c_train)\n",
    "            train_labels_comb.append(c_label)\n",
    "            \n",
    "        train_feats_comb = np.array(train_feats_comb)\n",
    "        train_labels_comb = np.array(train_labels_comb)\n",
    "        \n",
    "        train_feats2 = train_feats_comb\n",
    "        train_labels2 = train_labels_comb\n",
    "        print('augmentation: combined random images')\n",
    "    else:\n",
    "        train_feats2 = train_feats\n",
    "        train_labels2 = train_labels\n",
    "        \n",
    "    # data augmentation, flip vertically or horizontally\n",
    "    flip = np.random.choice([0, 1, 2, 3, 4])\n",
    "    if flip == 1: # horizontal flip\n",
    "        \n",
    "        train_feats2 = np.array([np.fliplr(i) for i in train_feats2])\n",
    "        train_labels2 = np.array([np.fliplr(i) for i in train_labels2])\n",
    "        print('flipped horizontally')\n",
    "        \n",
    "        k = np.random.choice([0, 1, 2, 3])\n",
    "        if k != 0:\n",
    "\n",
    "            train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "            train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "            \n",
    "#             train_feats2 = np.array([rotate(i, k, reshape=False) for i in train_feats2])\n",
    "#             train_labels2 = np.array([rotate(i, k, reshape=False) for i in train_labels2])\n",
    "            print(f'rotated {k*90} degrees')\n",
    "        \n",
    "    elif flip == 2:\n",
    "\n",
    "        train_feats2 = np.array([np.flipud(i) for i in train_feats2])\n",
    "        train_labels2 = np.array([np.flipud(i) for i in train_labels2])\n",
    "        print('flipped vertically')\n",
    "        \n",
    "        k = np.random.choice([0, 1, 2, 3])\n",
    "        if k != 0:\n",
    "\n",
    "            train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "            train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "            \n",
    "#             train_feats2 = np.array([rotate(i, k, reshape=False) for i in train_feats2])\n",
    "#             train_labels2 = np.array([rotate(i, k, reshape=False) for i in train_labels2])\n",
    "            print(f'rotated {k*90} degrees')\n",
    "            \n",
    "    elif flip == 3:\n",
    "\n",
    "        train_feats2 = np.array([np.fliplr(i) for i in train_feats2])\n",
    "        train_labels2 = np.array([np.fliplr(i) for i in train_labels2])\n",
    "        \n",
    "        train_feats2 = np.array([np.flipud(i) for i in train_feats2])\n",
    "        train_labels2 = np.array([np.flipud(i) for i in train_labels2])\n",
    "        print('flipped vertically and horizontally')\n",
    "        \n",
    "        k = np.random.choice([0, 1, 2, 3])\n",
    "        if k != 0:\n",
    "\n",
    "            train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "            train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "\n",
    "#             train_feats2 = np.array([rotate(i, k, reshape=False) for i in train_feats2])\n",
    "#             train_labels2 = np.array([rotate(i, k, reshape=False) for i in train_labels2])\n",
    "            print(f'rotated {k*90} degrees')\n",
    "            \n",
    "    else:\n",
    "#         train_feats2 = train_feats\n",
    "#         train_labels2 = train_labels\n",
    "        \n",
    "        k = np.random.choice([0, 1, 2, 3])\n",
    "        if k != 0:\n",
    "\n",
    "            train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "            train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "\n",
    "#             train_feats2 = np.array([rotate(i, k, reshape=False) for i in train_feats2])\n",
    "#             train_labels2 = np.array([rotate(i, k, reshape=False) for i in train_labels2])\n",
    "            print(f'rotated {k*90} degrees')        \n",
    "    # concatentate dataset\n",
    "    dataset = [train_feats2, train_labels2]\n",
    "    \n",
    "\n",
    "    # train model\n",
    "    train(unet_model, dataset, n_epochs=1, n_batch=1)\n",
    "\n",
    "#     print(i)\n",
    "    if j % 5 == 0:\n",
    "        # save the generator model\n",
    "        unet_model.save(f\"{filename_g}_{j}.h5\")\n",
    "        print(f'saved generator model in {filename_g}_{j}.h5')\n",
    "\n",
    "\n",
    "    # check on train\n",
    "    for i in np.random.choice(list(range(len(dataset[0]))), 5):\n",
    "\n",
    "        f, axs = plt.subplots(nrows=1, ncols=3, figsize=(5, 2))\n",
    "\n",
    "        train_feat = train_feats2[i]\n",
    "        train_true = train_labels2[i]\n",
    "        train_true = compress_label(train_true)\n",
    "\n",
    "        # show one input\n",
    "        axs[0].imshow(train_feat[:, :, 0])\n",
    "        \n",
    "        # show true\n",
    "        axs[1].imshow(train_true)\n",
    "\n",
    "        # predict sample with values from 0 to 1\n",
    "        train_pred = unet_model.predict(np.reshape(train_feat, (1, 128, 128, 10)))\n",
    "        train_pred = compress_label(train_pred[0])\n",
    "\n",
    "        axs[2].imshow(train_pred)\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        print(np.max(train_true), np.max(train_pred))\n",
    "        print(np.min(train_true), np.min(train_pred))\n",
    "        print(np.mean(train_true), np.mean(train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check on train\n",
    "for i in np.random.choice(list(range(len(dataset[0]))), 5):\n",
    "\n",
    "    f, axs = plt.subplots(nrows=1, ncols=3, figsize=(5, 2))\n",
    "\n",
    "    train_feat = train_feats[i]\n",
    "    train_true = train_labels[i]\n",
    "    train_true = compress_label(train_true)\n",
    "\n",
    "    # show one input\n",
    "    axs[0].imshow(train_feat[:, :, 0])\n",
    "\n",
    "    # show true\n",
    "    axs[1].imshow(train_true)\n",
    "\n",
    "    # predict sample with values from 0 to 1\n",
    "    train_pred = unet_model.predict(np.reshape(train_feat, (1, 128, 128, 10)))\n",
    "    train_pred = compress_label(train_pred[0])\n",
    "\n",
    "    axs[2].imshow(train_pred)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(np.max(train_true), np.max(train_pred))\n",
    "    print(np.min(train_true), np.min(train_pred))\n",
    "    print(np.mean(train_true), np.mean(train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get n random images from all images in the dataset\n",
    "# # for 128 X 128 X 1 output\n",
    "    \n",
    "# filename_g = f'models/datos_unet_gen_sigmoid_{loss}_{activation}_{today}'\n",
    "\n",
    "# for i in range(10000):\n",
    "    \n",
    "#     # data augmentation, flip vertically or horizontally\n",
    "#     flip = np.random.choice([0, 1, 2, 3, 4])\n",
    "#     if flip == 1: # horizontal flip\n",
    "#         print('flipped horizontally')\n",
    "        \n",
    "#         train_feats2 = np.array([np.fliplr(i) for i in train_feats])\n",
    "#         train_labels2 = np.array([np.fliplr(i) for i in train_labels])\n",
    "        \n",
    "#         k = np.random.choice([0, 1, 2, 3])\n",
    "#         if k != 0:\n",
    "#             print(f'rotated {k*90} degrees')\n",
    "\n",
    "#             train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "#             train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "        \n",
    "#     elif flip == 2:\n",
    "#         print('flipped vertically')\n",
    "#         train_feats2 = np.array([np.flipud(i) for i in train_feats])\n",
    "#         train_labels2 = np.array([np.flipud(i) for i in train_labels])\n",
    "        \n",
    "#         k = np.random.choice([0, 1, 2, 3])\n",
    "#         if k != 0:\n",
    "#             print(f'rotated {k*90} degrees')\n",
    "\n",
    "#             train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "#             train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "        \n",
    "#     elif flip == 3:\n",
    "#         print('flipped vertically and horizontally')\n",
    "#         train_feats2 = np.array([np.fliplr(i) for i in train_feats])\n",
    "#         train_labels2 = np.array([np.fliplr(i) for i in train_labels])\n",
    "        \n",
    "#         train_feats2 = np.array([np.flipud(i) for i in train_feats2])\n",
    "#         train_labels2 = np.array([np.flipud(i) for i in train_labels2])\n",
    "        \n",
    "#         k = np.random.choice([0, 1, 2, 3])\n",
    "#         if k != 0:\n",
    "#             print(f'rotated {k*90} degrees')\n",
    "\n",
    "#             train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "#             train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "        \n",
    "#     else:\n",
    "#         train_feats2 = train_feats\n",
    "#         train_labels2 = train_labels\n",
    "        \n",
    "#         k = np.random.choice([0, 1, 2, 3])\n",
    "#         if k != 0:\n",
    "#             print(f'rotated {k*90} degrees')\n",
    "\n",
    "#             train_feats2 = np.array([np.rot90(i, k=k) for i in train_feats2])\n",
    "#             train_labels2 = np.array([np.rot90(i, k=k) for i in train_labels2])\n",
    "        \n",
    "#     # concatentate dataset\n",
    "#     dataset = [train_feats2, train_labels2]\n",
    "    \n",
    "\n",
    "#     # train model\n",
    "#     train(unet_model, dataset, n_epochs=1, n_batch=1)\n",
    "\n",
    "#     if i % 20 == 0:\n",
    "#         # save the generator model\n",
    "#         unet_model.save(f\"{filename_g}_{i}.h5\")\n",
    "#         print(f'saved generator model in {filename_g}_{i}.h5')\n",
    "\n",
    "\n",
    "#     # check on train\n",
    "#     for i in np.random.choice(list(range(len(dataset[0]))), 5):\n",
    "\n",
    "#         f, axs = plt.subplots(nrows=1, ncols=3, figsize=(5, 2))\n",
    "\n",
    "#         train_feat = train_feats2[i]\n",
    "#         train_true = train_labels2[i]\n",
    "\n",
    "#         # show one input\n",
    "#         axs[0].imshow(train_feat[:, :, 0])\n",
    "        \n",
    "#         # show true\n",
    "#         axs[1].imshow(train_true)\n",
    "\n",
    "#         # predict sample with values from 0 to 1\n",
    "#         train_pred = unet_model.predict(np.reshape(train_feat, (1, 128, 128, 10)))\n",
    "\n",
    "#         axs[2].imshow(train_pred[0][:, :, 0])\n",
    "\n",
    "#         plt.show()\n",
    "        \n",
    "#         print(np.max(train_true), np.max(train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
